# ðŸŒŸ SAM-Replication PyTorch Implementation

This repository contains a replication of **SAM-ResNet18**, integrating **Sharpness-Aware Minimization (SAM)** with a **ResNet-18 backbone**, using PyTorch. The model is designed for **robust and efficient image classification**, applying **perturbations on model weights** to improve generalization.

- Implemented **ResNet-18** with **residual blocks** and **SAM perturbation hooks**.  
- Architecture:  
**Stem â†’ ResNet-18 Blocks â†’ GlobalAvgPool â†’ Flatten â†’ FC**

> **Note on SAM:** Our implementation injects perturbations in model parameters according to the SAM algorithm (`rho` scaling). The wrapper optimizer handles the **perturbation â†’ step â†’ restore** cycle.  

**Paper reference:** [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/abs/2010.01412) âš¡

---

## ðŸ–¼ Overview â€“ SAM + ResNet-18 Architecture

![Overview](images/figmix.jpg)  

- *Figure 1* shows that with SAM we reach â€œwide & flatâ€ minima â€” small changes to weights donâ€™t blow up the loss â†’ this helps in achieving better test performance and robustness against small perturbations.  
- *Figure 2* visualizes the twoâ€‘step SAM update: first a small perturbation is added to weights, then after reâ€‘evaluating the loss, update is applied â€” this effectively searches not just for a lowâ€‘loss point but for a stable region (â€œneighborhoodâ€) in parameter space.  
- *Table 1* (from the original paper) reports consistent improvements in test error across datasets (CIFARâ€‘10/100, ImageNet, etc.) when using SAM â€” confirming that flat minima translate into real gains in generalization and robustness.  


> **Model highlights:**  
> - Residual connections in ResNet-18 allow gradient flow and prevent vanishing.  
> - SAM perturbation improves generalization without adding extra parameters.  
> - Adaptive pooling + flatten ensures consistent input to FC layer.

---

## ðŸ“‹ Project Structure

```bash
SAM-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py             # Standard conv layer
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py          # Flatten layer
â”‚   â”‚   â”œâ”€â”€ fc_layer.py               # Fully connected layer (num_classes)
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py      # MaxPool
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py      # AdaptiveAvgPool
â”‚   â”‚   â”œâ”€â”€ sam_perturbation.py       # SAM perturbation calculation (rho, epsilon)
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â””â”€â”€ resnet_block.py           # ResNet BasicBlock / BottleneckBlock
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ sam_resnet18.py           # Full model: Stem + ResNet-18 blocks + Classifier + SAM hooks
â”‚   â”‚
â”‚   â”œâ”€â”€ optimizers/
â”‚   â”‚   â””â”€â”€ sam_optimizer.py          # SAM optimizer wrapper (base_optimizer + perturbation)
â”‚   â”‚
â”‚   â””â”€â”€ config.py                      # Input size, num_classes, rho, optimizer settings
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                     # Figures illustrating SAM + ResNet18
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
